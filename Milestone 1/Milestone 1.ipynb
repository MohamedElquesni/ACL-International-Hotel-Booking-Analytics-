{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MohamedElquesni/ACL-International-Hotel-Booking-Analytics-/blob/Mohamed/Milestone%201/Milestone%201.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "15be22039cc67b",
      "metadata": {
        "id": "15be22039cc67b"
      },
      "source": [
        "# Milestone 1: International Hotel Booking Analytics\n",
        "## Predicting Hotel Country Groups using Machine Learning (ML)\n",
        "\n",
        "**Team Number:** [90]  \n",
        "---\n",
        "\n",
        "## Project Overview\n",
        "\n",
        "**Goal:** Build a multi-class classification model to predict the country group of hotels based on user demographics, hotel characteristics, and review scores.\n",
        "\n",
        "**Dataset:**\n",
        "- 50,000 reviews across 25 hotels in 25 countries\n",
        "- 2,000 unique users with demographic information\n",
        "- 11 target country groups\n",
        "\n",
        "**Deliverables:**\n",
        "1. A cleaned dataset after the feature engineering step.\n",
        "2. Data engineering insights, including:\n",
        " - The best city for each traveller type.\n",
        " - The top three countries with the best value-for-money scores per traveller age group.\n",
        "\n",
        "3. A trained classification model (statistical ML or shallow FFNN).\n",
        "4. Model interpretation and explainability through XAI techniques (SHAP and LIME).\n",
        "5. An inference function.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "l3zc9ubxxcn",
      "metadata": {
        "id": "l3zc9ubxxcn"
      },
      "source": [
        "# Section 1: Data Cleaning"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "29f4d60a768f2aa5",
      "metadata": {
        "id": "29f4d60a768f2aa5"
      },
      "source": [
        "## 1.1 - Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "c8c75190c8840e03",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-10-23T15:23:50.898281Z",
          "start_time": "2025-10-23T15:23:50.895104Z"
        },
        "id": "c8c75190c8840e03"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from matplotlib import pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "457ab6f4d5975ae6",
      "metadata": {
        "id": "457ab6f4d5975ae6"
      },
      "source": [
        "## 1.2 - Loading and Assessing Datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "da2b4b6bbc45af6e",
      "metadata": {
        "id": "da2b4b6bbc45af6e"
      },
      "source": [
        "### Hotels Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d67cad12cf9b507f",
      "metadata": {
        "id": "d67cad12cf9b507f"
      },
      "source": [
        "#### Loading the Hotels Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "78c0881235ce70aa",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-10-23T15:23:50.919892Z",
          "start_time": "2025-10-23T15:23:50.907522Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "id": "78c0881235ce70aa",
        "outputId": "2191da46-8327-4e9e-a768-348ecd06ec66"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/hotels.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-215952059.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_hotels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/hotels.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf_hotels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/hotels.csv'"
          ]
        }
      ],
      "source": [
        "df_hotels = pd.read_csv('/content/hotels.csv')\n",
        "df_hotels.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f334ac37d1767347",
      "metadata": {
        "id": "f334ac37d1767347"
      },
      "source": [
        "#### Renaming Hotel Columns --> Hotel_ + Original Column name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "942cd43e249f92a2",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-10-23T15:23:50.954076Z",
          "start_time": "2025-10-23T15:23:50.951433Z"
        },
        "id": "942cd43e249f92a2"
      },
      "outputs": [],
      "source": [
        "df_hotels.columns = [\n",
        "    col if col == 'hotel_id' or col == 'hotel_name' else 'hotel_' + col\n",
        "    for col in df_hotels.columns\n",
        "]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1e195b3a3c0b4f0",
      "metadata": {
        "id": "1e195b3a3c0b4f0"
      },
      "source": [
        "#### Checking for Null Values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "86c9e54c7878d44e",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-10-23T15:23:51.010049Z",
          "start_time": "2025-10-23T15:23:51.005926Z"
        },
        "id": "86c9e54c7878d44e"
      },
      "outputs": [],
      "source": [
        "df_hotels.isnull().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "80fd038b39e4a3d2",
      "metadata": {
        "id": "80fd038b39e4a3d2"
      },
      "source": [
        "#### Checking for Duplicated Values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44250c36f1247d35",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-10-23T15:23:51.084537Z",
          "start_time": "2025-10-23T15:23:51.080803Z"
        },
        "id": "44250c36f1247d35"
      },
      "outputs": [],
      "source": [
        "df_hotels.duplicated().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "844c2dc1ca08fd4f",
      "metadata": {
        "id": "844c2dc1ca08fd4f"
      },
      "source": [
        "### Reviews Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c61f1fcfc2b68a95",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-10-23T15:23:51.237185Z",
          "start_time": "2025-10-23T15:23:51.158563Z"
        },
        "id": "c61f1fcfc2b68a95"
      },
      "outputs": [],
      "source": [
        "df_reviews = pd.read_csv('/content/reviews.csv')\n",
        "df_reviews.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9129802ac37b080c",
      "metadata": {
        "id": "9129802ac37b080c"
      },
      "source": [
        "#### Renaming Reviews Columns --> Review_ + Original Column name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2b814f510627729a",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-10-23T15:23:51.268855Z",
          "start_time": "2025-10-23T15:23:51.266316Z"
        },
        "id": "2b814f510627729a"
      },
      "outputs": [],
      "source": [
        "df_reviews.columns = [\n",
        "    col if col == 'review_id'\n",
        "           or col == 'user_id'\n",
        "           or col == 'hotel_id'\n",
        "           or col == 'review_date'\n",
        "           or col == 'review_text'\n",
        "        else 'review_' + col\n",
        "    for col in df_reviews.columns\n",
        "]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ae4a33a122340181",
      "metadata": {
        "id": "ae4a33a122340181"
      },
      "source": [
        "#### Checking for Null Values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "959605430ea81098",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-10-23T15:23:51.330382Z",
          "start_time": "2025-10-23T15:23:51.321357Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "959605430ea81098",
        "outputId": "a5215221-256f-48a4-abed-6ff931b5edcc"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'df_reviews' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3166299896.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_reviews\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'df_reviews' is not defined"
          ]
        }
      ],
      "source": [
        "df_reviews.isnull().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c7d2051cc9e3c256",
      "metadata": {
        "id": "c7d2051cc9e3c256"
      },
      "source": [
        "#### Checking for Duplicated Values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "865cab3e8172f8df",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-10-23T15:23:51.429826Z",
          "start_time": "2025-10-23T15:23:51.401221Z"
        },
        "id": "865cab3e8172f8df"
      },
      "outputs": [],
      "source": [
        "df_reviews.duplicated().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7c8d80b1ce6b9a0d",
      "metadata": {
        "id": "7c8d80b1ce6b9a0d"
      },
      "source": [
        "### Users Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "923d039f80276f2e",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-10-23T15:23:51.509145Z",
          "start_time": "2025-10-23T15:23:51.500173Z"
        },
        "id": "923d039f80276f2e"
      },
      "outputs": [],
      "source": [
        "df_users = pd.read_csv('/content/users.csv')\n",
        "df_users.head() # This shows the 5 rows"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1460d25b57366803",
      "metadata": {
        "id": "1460d25b57366803"
      },
      "source": [
        "#### Renaming Users Columns --> User_ + Original Column name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b3c01d093baa02c",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-10-23T15:23:51.564471Z",
          "start_time": "2025-10-23T15:23:51.562139Z"
        },
        "id": "6b3c01d093baa02c"
      },
      "outputs": [],
      "source": [
        "df_users.columns = [\n",
        "    col if col == 'user_id'\n",
        "        or col == 'user_gender'\n",
        "        else 'user_' + col\n",
        "    for col in df_users.columns\n",
        "]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2873e9b40c517b7f",
      "metadata": {
        "id": "2873e9b40c517b7f"
      },
      "source": [
        "#### Checking for Null Values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "df35bd0bd7e55dd1",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-10-23T15:23:51.636172Z",
          "start_time": "2025-10-23T15:23:51.631964Z"
        },
        "id": "df35bd0bd7e55dd1"
      },
      "outputs": [],
      "source": [
        "df_users.isnull().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9d8e3acff57e1f7d",
      "metadata": {
        "id": "9d8e3acff57e1f7d"
      },
      "source": [
        "#### Checking for Duplicated Values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d772189ce05d1e5",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-10-23T15:23:51.712805Z",
          "start_time": "2025-10-23T15:23:51.708501Z"
        },
        "id": "2d772189ce05d1e5"
      },
      "outputs": [],
      "source": [
        "df_users.duplicated().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8n4trrda6ih",
      "metadata": {
        "id": "8n4trrda6ih"
      },
      "source": [
        "## 1.3 - Merging Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "16qtsi5p0z9",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-10-23T15:23:51.840135Z",
          "start_time": "2025-10-23T15:23:51.808687Z"
        },
        "id": "16qtsi5p0z9"
      },
      "outputs": [],
      "source": [
        "# Merge reviews with users on user_id\n",
        "df_merged = pd.merge(df_reviews, df_users, on='user_id', how='left')\n",
        "\n",
        "# Merge the result with hotels on hotel_id\n",
        "df_merged = pd.merge(df_merged, df_hotels, on='hotel_id', how='left')\n",
        "\n",
        "df_merged.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "yyju46d246",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-10-23T15:23:51.921164Z",
          "start_time": "2025-10-23T15:23:51.898785Z"
        },
        "id": "yyju46d246"
      },
      "outputs": [],
      "source": [
        "df_merged.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "xbFSH6CAlDIs",
      "metadata": {
        "id": "xbFSH6CAlDIs"
      },
      "source": [
        "## 1.4 - Dropping Unnecessary Columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "SE_a-gOolLjq",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-10-23T15:23:52.005870Z",
          "start_time": "2025-10-23T15:23:51.985743Z"
        },
        "id": "SE_a-gOolLjq"
      },
      "outputs": [],
      "source": [
        "# Drop unnecessary columns that do not contribute to the predictive modeling task\n",
        "# These columns are either textual, identifiers or dates that add no generalizable value\n",
        "\n",
        "df_merged.drop(\n",
        "    columns=[\n",
        "        'review_date',\n",
        "        'review_text',\n",
        "        'user_join_date',\n",
        "        'hotel_name'\n",
        "    ],\n",
        "    inplace=True\n",
        ")\n",
        "\n",
        "df_merged.info()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "izurjlail9",
      "metadata": {
        "id": "izurjlail9"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "x9srolp5pmd",
      "metadata": {
        "id": "x9srolp5pmd"
      },
      "source": [
        "## 1.5 - Cleaned Dataset Summary\n",
        "\n",
        "All datasets have been successfully loaded, cleaned, and merged:\n",
        "- No null values found.\n",
        "- No duplicate records.\n",
        "- Columns renamed with prefixes for clarity.\n",
        "- Unnecessary columns were dropped.\n",
        "- Final merged dataset contains 50,000 reviews with complete hotel characteristics and user information\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6d995c8fe1704d8c",
      "metadata": {
        "id": "6d995c8fe1704d8c"
      },
      "source": [
        "# Section 2: Data Engineering Questions\n",
        "\n",
        "Using the cleaned and merged dataset, we analyze and visualize the following:\n",
        "\n",
        "1. **Best City for Each Trave;ler Type**\n",
        "   - Identify the city with the highest average review score for each traveler type.\n",
        "\n",
        "2. **Top 3 Countries by Value-for-Money per Age Group**\n",
        "   - Find the top 3 countries with the highest value-for-money score per traveller’s age group."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fb9bfb2d238876c2",
      "metadata": {
        "id": "fb9bfb2d238876c2"
      },
      "source": [
        "## 2.1 - Best City for Each Traveller Type"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c1299c64fe454b4c",
      "metadata": {
        "id": "c1299c64fe454b4c"
      },
      "source": [
        "### Heatmap Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "99b8c1786515eda1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "99b8c1786515eda1",
        "outputId": "e24f4511-8ba5-4c1c-84d5-4c3ac017f511"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'df_merged' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2198074625.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m pivot = pd.pivot_table(\n\u001b[0;32m----> 2\u001b[0;31m         \u001b[0mdf_merged\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m         \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"hotel_city\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"user_traveller_type\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mvalues\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"review_score_overall\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'df_merged' is not defined"
          ]
        }
      ],
      "source": [
        "pivot = pd.pivot_table(\n",
        "        df_merged,\n",
        "        index=\"hotel_city\",\n",
        "        columns=\"user_traveller_type\",\n",
        "        values=\"review_score_overall\",\n",
        "        aggfunc=\"mean\"\n",
        "    )\n",
        "\n",
        "plt.figure(figsize = (12,6))\n",
        "\n",
        "\n",
        "sns.heatmap(pivot, annot=True, cmap=\"copper\", fmt=\".2f\", linewidths=0.5)\n",
        "\n",
        "# Extra GUI enhancements\n",
        "plt.title(\"Average Review Score per City and Traveller Type \", fontsize=14)\n",
        "plt.xlabel(\"Traveller Type\")\n",
        "plt.ylabel(\"City\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "54d0649549c4b2cb",
      "metadata": {
        "id": "54d0649549c4b2cb"
      },
      "source": [
        "### Insights\n",
        "\n",
        "Using the heatmap visualization, we can observe clear differences in average review scores across traveller types and cities:\n",
        "\n",
        "- **Business travellers:** Dubai achieved the highest average score of **8.97**.\n",
        "- **Couples:** Amsterdam recorded the highest average score of **9.10**.\n",
        "- **Families:** Dubai again stood out with an average score of **9.21**.\n",
        "- **Solo travellers:** Amsterdam had the highest average score of **9.11**."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d99eacbd40f5d9d",
      "metadata": {
        "id": "3d99eacbd40f5d9d"
      },
      "source": [
        "## 2.2 - Top 3 Countries by Value-for-Money per Age Group"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "73294be7819bf89b",
      "metadata": {
        "id": "73294be7819bf89b"
      },
      "source": [
        "### Heatmap Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a630c307de2f148",
      "metadata": {
        "id": "4a630c307de2f148"
      },
      "outputs": [],
      "source": [
        "pivot = pd.pivot_table(\n",
        "    df_merged,\n",
        "    index=\"hotel_country\",\n",
        "    columns=\"user_age_group\",\n",
        "    values=\"review_score_value_for_money\",\n",
        "    aggfunc=\"mean\"\n",
        ")\n",
        "\n",
        "plt.figure(figsize=(14, 6))\n",
        "\n",
        "sns.heatmap(\n",
        "    pivot,\n",
        "    annot=True,\n",
        "    cmap=\"copper\",\n",
        "    fmt=\".2f\",\n",
        "    linewidths=0.5\n",
        ")\n",
        "\n",
        "# Extra GUI Interface\n",
        "plt.title(\"Average Value-for-Money Score per Country and Age Group\", fontsize=14)\n",
        "plt.xlabel(\"User Age Group\")\n",
        "plt.ylabel(\"Hotel Country\")\n",
        "plt.yticks(rotation=0)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2a51c8b21678adf",
      "metadata": {
        "id": "2a51c8b21678adf"
      },
      "source": [
        "### Insights\n",
        "\n",
        "Top 3 countries by value-for-money score for each age group:\n",
        "\n",
        "- **18–24:** China (8.71), Netherlands (8.70), Canada (8.66)\n",
        "- **25–34:** China (8.73), Netherlands (8.68), Spain (8.63)\n",
        "- **35–44:** China (8.70), Netherlands (8.69), New Zealand (8.65)\n",
        "- **45–54:** China (8.72), New Zealand (8.67), Netherlands (8.65)\n",
        "- **55+:** Netherlands (8.70), New Zealand (8.63), China (8.60)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "lzcsrv4a8tm",
      "metadata": {
        "id": "lzcsrv4a8tm"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "494e5404653157f8",
      "metadata": {
        "id": "494e5404653157f8"
      },
      "source": [
        "# Section 3: Exploratory Data Analysis (EDA)\n",
        "\n",
        "**Objective:** Understand data distributions, correlations, and patterns to inform feature engineering and modeling decisions.\n",
        "\n",
        "---\n",
        "\n",
        "## 3.1 - Target Variable Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2dd2b6a1b9d06f51",
      "metadata": {
        "id": "2dd2b6a1b9d06f51"
      },
      "source": [
        "### Create Target Variable\n",
        "\n",
        "We group the 25 countries into 11 geographic regions (country groups) to create our classification target."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "6ec64beb990963cc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "6ec64beb990963cc",
        "jupyter": {
          "is_executing": true
        },
        "outputId": "2210bc81-a2b6-469f-85e7-da3df11f8ef7"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'df_merged' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1553136294.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;31m# Apply the mapping to create our target variable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m \u001b[0mdf_merged\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'country_group'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_merged\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'hotel_country'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcountry_to_group\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0mdf_merged\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'df_merged' is not defined"
          ]
        }
      ],
      "source": [
        "# Create a mapping dictionary from country names to geographic regions (country groups)\n",
        "# This groups the 25 countries into 11 regions (country groups) for classification\n",
        "country_to_group = {\n",
        "    # North America\n",
        "    'United States': 'North_America',\n",
        "    'Canada': 'North_America',\n",
        "\n",
        "    # Western Europe\n",
        "    'Germany': 'Western_Europe',\n",
        "    'France': 'Western_Europe',\n",
        "    'United Kingdom': 'Western_Europe',\n",
        "    'Netherlands': 'Western_Europe',\n",
        "    'Spain': 'Western_Europe',\n",
        "    'Italy': 'Western_Europe',\n",
        "\n",
        "    # Eastern Europe\n",
        "    'Russia': 'Eastern_Europe',\n",
        "\n",
        "    # East Asia\n",
        "    'China': 'East_Asia',\n",
        "    'Japan': 'East_Asia',\n",
        "    'South Korea': 'East_Asia',\n",
        "\n",
        "    # Southeast Asia\n",
        "    'Thailand': 'Southeast_Asia',\n",
        "    'Singapore': 'Southeast_Asia',\n",
        "\n",
        "    # Middle East\n",
        "    'United Arab Emirates': 'Middle_East',\n",
        "    'Turkey': 'Middle_East',\n",
        "\n",
        "    # Africa\n",
        "    'Egypt': 'Africa',\n",
        "    'Nigeria': 'Africa',\n",
        "    'South Africa': 'Africa',\n",
        "\n",
        "    # Oceania\n",
        "    'Australia': 'Oceania',\n",
        "    'New Zealand': 'Oceania',\n",
        "\n",
        "    # South America\n",
        "    'Brazil': 'South_America',\n",
        "    'Argentina': 'South_America',\n",
        "\n",
        "    # South Asia\n",
        "    'India': 'South_Asia',\n",
        "\n",
        "    # North America (Mexico separate due to different characteristics)\n",
        "    'Mexico': 'North_America_Mexico'\n",
        "}\n",
        "\n",
        "# Apply the mapping to create our target variable\n",
        "df_merged['country_group'] = df_merged['hotel_country'].map(country_to_group)\n",
        "\n",
        "df_merged.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "nsc4tlaaakn",
      "metadata": {
        "id": "nsc4tlaaakn"
      },
      "outputs": [],
      "source": [
        "# Count how many reviews belong to each country group\n",
        "print(\"Distribution of reviews across country groups:\")\n",
        "print(df_merged['country_group'].value_counts().sort_index())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "752f17d4c4f520c7",
      "metadata": {
        "id": "752f17d4c4f520c7"
      },
      "source": [
        "### Visualize Target Distribution\n",
        "\n",
        "Now let's visualize the distribution to better understand class imbalance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "gbw1ah0t21",
      "metadata": {
        "id": "gbw1ah0t21"
      },
      "outputs": [],
      "source": [
        "# Get the distribution sorted by count (descending)\n",
        "target_dist = df_merged['country_group'].value_counts().sort_values(ascending=False)\n",
        "\n",
        "# Create a figure with 2 subplots side by side\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
        "\n",
        "# First plot: Bar chart showing counts\n",
        "target_dist.plot(kind='bar', ax=axes[0], color='steelblue')\n",
        "axes[0].set_title('Distribution of Reviews by Country Group', fontsize=14)\n",
        "axes[0].set_xlabel('Country Group')\n",
        "axes[0].set_ylabel('Number of Reviews')\n",
        "axes[0].tick_params(axis='x', rotation=45)\n",
        "\n",
        "# Second plot: Pie chart showing percentages\n",
        "axes[1].pie(target_dist, labels=target_dist.index, startangle=90)\n",
        "axes[1].set_title('Percentage Distribution by Country Group', fontsize=14)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Calculate and display class imbalance statistics\n",
        "largest_class = target_dist.idxmax()\n",
        "smallest_class = target_dist.idxmin()\n",
        "imbalance_ratio = target_dist.max() / target_dist.min()\n",
        "\n",
        "print(f\"\\nClass Imbalance Analysis:\")\n",
        "print(f\"Largest class: {largest_class} with {target_dist.max()} samples\")\n",
        "print(f\"Smallest class: {smallest_class} with {target_dist.min()} samples\")\n",
        "print(f\"Imbalance ratio: {imbalance_ratio:.2f}:1\")\n",
        "print(f\"\\nThis means the largest class has {imbalance_ratio:.2f}x more samples than the smallest class.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "552cme8jtoe",
      "metadata": {
        "id": "552cme8jtoe"
      },
      "source": [
        "---\n",
        "\n",
        "## 3.2 - Numerical Features Analysis\n",
        "\n",
        "Let's analyze the distribution and statistics of numerical features to understand their behavior."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5dx3en1s9qu",
      "metadata": {
        "id": "5dx3en1s9qu"
      },
      "outputs": [],
      "source": [
        "# Identify all numerical columns in the dataset\n",
        "numerical_cols = df_merged.select_dtypes(include=[np.number]).columns.tolist()\n",
        "\n",
        "print(f\"Total numerical features: {len(numerical_cols)}\\n\")\n",
        "print(\"List of numerical features:\")\n",
        "for i, col in enumerate(numerical_cols, 1):\n",
        "    print(f\"  {i}. {col}\")\n",
        "\n",
        "print(\"\\n\")\n",
        "print(\"Statistical Summary of Numerical Features:\")\n",
        "print(\"\\n\")\n",
        "df_merged[numerical_cols].describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dam1d2sjzra",
      "metadata": {
        "id": "dam1d2sjzra"
      },
      "outputs": [],
      "source": [
        "# Visualize the distribution of numerical features using histograms\n",
        "# This helps us understand if features are normally distributed, skewed, etc.\n",
        "\n",
        "fig, axes = plt.subplots(4, 4, figsize=(18, 16))\n",
        "axes = axes.ravel()  # Flatten the 2D array to 1D for easier iteration\n",
        "\n",
        "# Plot histogram for each numerical column (first 16 columns)\n",
        "for idx, col in enumerate(numerical_cols[:16]):\n",
        "    axes[idx].hist(df_merged[col], bins=30, color='skyblue', edgecolor='black', alpha=0.7)\n",
        "    axes[idx].set_title(f'{col}', fontsize=10, fontweight='bold')\n",
        "    axes[idx].set_xlabel('Value', fontsize=9)\n",
        "    axes[idx].set_ylabel('Frequency', fontsize=9)\n",
        "    axes[idx].grid(axis='y', alpha=0.3)\n",
        "\n",
        "plt.suptitle('Distribution of Numerical Features', fontsize=16, fontweight='bold', y=1.00)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "99i76n304gi",
      "metadata": {
        "id": "99i76n304gi"
      },
      "outputs": [],
      "source": [
        "# Use box plots to detect outliers in numerical features\n",
        "# Box plots show: median (center line), quartiles (box edges), and outliers (dots beyond whiskers)\n",
        "\n",
        "fig, axes = plt.subplots(4, 4, figsize=(18, 16))\n",
        "axes = axes.ravel()\n",
        "\n",
        "plot_idx = 0  # Separate counter for axes indexing\n",
        "# Create box plot for each numerical column\n",
        "for idx, col in enumerate(numerical_cols[:19]):\n",
        "    if col == 'review_id' or col == 'user_id' or col == 'hotel_id':\n",
        "        continue\n",
        "    axes[plot_idx].boxplot(df_merged[col], vert=True)\n",
        "    axes[plot_idx].set_title(f'{col}', fontsize=10, fontweight='bold')\n",
        "    axes[plot_idx].set_ylabel('Value', fontsize=9)\n",
        "    axes[plot_idx].grid(axis='y', alpha=0.3)\n",
        "    plot_idx += 1\n",
        "\n",
        "plt.suptitle('Box Plots for Outlier Detection', fontsize=16, fontweight='bold', y=1.00)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n Insights:\")\n",
        "print(\"- The box represents the interquartile range (IQR): 25th to 75th percentile\")\n",
        "print(\"- The line inside the box is the median (50th percentile)\")\n",
        "print(\"- Whiskers extend to 1.5 * IQR from the box\")\n",
        "print(\"- Points beyond whiskers are potential outliers\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cbwwmygbuxe",
      "metadata": {
        "id": "cbwwmygbuxe"
      },
      "source": [
        "## 3.3 - Correlation Analysis\n",
        "\n",
        "Correlation analysis helps us identify relationships between numerical features. High correlation can indicate:\n",
        "- Redundant features (multicollinearity)\n",
        "- Features that move together\n",
        "- Potential feature combinations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "89cinwbpf9j",
      "metadata": {
        "id": "89cinwbpf9j"
      },
      "outputs": [],
      "source": [
        "# Calculate correlation matrix (Pearson correlation coefficient)\n",
        "# Values range from -1 (perfect negative correlation) to +1 (perfect positive correlation)\n",
        "\n",
        "numerical_cols = [col for col in numerical_cols\n",
        "                           if col not in ['review_id', 'user_id', 'hotel_id']]\n",
        "\n",
        "correlation_matrix = df_merged[numerical_cols].corr()\n",
        "\n",
        "# Visualize using a heatmap\n",
        "plt.figure(figsize=(14, 12))\n",
        "sns.heatmap(\n",
        "    correlation_matrix,\n",
        "    annot=True,\n",
        "    cmap='coolwarm',\n",
        "    center=0,\n",
        "    square=True,\n",
        "    linewidths=0.5,\n",
        "    cbar_kws={'label': 'Correlation Coefficient'}\n",
        ")\n",
        "plt.title('Correlation Matrix Heatmap', fontsize=16, fontweight='bold', pad=20)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Find and print highly correlated pairs (correlation > 0.8 or < -0.8)\n",
        "print(\"\\n\")\n",
        "print(\"Highly Correlated Feature Pairs (|correlation| > 0.8):\")\n",
        "print(\"\\n\")\n",
        "print(f\"{'Feature 1':<35} {'Feature 2':<35} {'Correlation':>10}\")\n",
        "print(\"-\"*80)\n",
        "\n",
        "highly_correlated_pairs = []\n",
        "for i in range(len(correlation_matrix.columns)):\n",
        "    for j in range(i+1, len(correlation_matrix.columns)):\n",
        "        corr_value = correlation_matrix.iloc[i, j]\n",
        "        if abs(corr_value) > 0.8:\n",
        "            feat1 = correlation_matrix.columns[i]\n",
        "            feat2 = correlation_matrix.columns[j]\n",
        "            print(f\"{feat1:<35} {feat2:<35} {corr_value:>10.3f}\")\n",
        "            highly_correlated_pairs.append((feat1, feat2, corr_value))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "mbbo11sm04f",
      "metadata": {
        "id": "mbbo11sm04f"
      },
      "source": [
        "## 3.4 - Categorical Features Analysis\n",
        "\n",
        "Let's examine the distribution of categorical features to understand user demographics and traveller characteristics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aur2pyzgbxi",
      "metadata": {
        "id": "aur2pyzgbxi"
      },
      "outputs": [],
      "source": [
        "# Define categorical features to analyze\n",
        "categorical_features = ['user_gender', 'user_age_group', 'user_traveller_type','user_country']\n",
        "\n",
        "print(\"CATEGORICAL FEATURES DISTRIBUTION\")\n",
        "\n",
        "for feat in categorical_features:\n",
        "    print(f\"\\n{feat.upper().replace('_', ' ')}:\")\n",
        "    print(\"-\" * 40)\n",
        "    counts = df_merged[feat].value_counts()\n",
        "\n",
        "    # Display counts and percentages\n",
        "    for value, count in counts.items():\n",
        "        percentage = (count / len(df_merged)) * 100\n",
        "        print(f\"  {value:<30} {count:>6} ({percentage:>5.2f}%)\")\n",
        "\n",
        "    print(f\"\\n  Total unique values: {df_merged[feat].nunique()}\")\n",
        "    print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "av26a1qgio",
      "metadata": {
        "id": "av26a1qgio"
      },
      "outputs": [],
      "source": [
        "# Visualize categorical features using bar charts\n",
        "fig, axes = plt.subplots(2, 2, figsize=(20, 15))\n",
        "axes = axes.ravel()\n",
        "\n",
        "# Create a bar chart for each categorical feature\n",
        "for idx, col in enumerate(categorical_features):\n",
        "    # Get value counts and plot\n",
        "    value_counts = df_merged[col].value_counts()\n",
        "    value_counts.plot(kind='bar', ax=axes[idx], color='teal', alpha=0.7, edgecolor='black')\n",
        "\n",
        "    # Formatting\n",
        "    axes[idx].set_title(f'{col.replace(\"_\", \" \").title()} Distribution',\n",
        "                        fontsize=13, fontweight='bold')\n",
        "    axes[idx].set_xlabel(col.replace('_', ' ').title(), fontsize=11)\n",
        "    axes[idx].set_ylabel('Count', fontsize=11)\n",
        "    axes[idx].tick_params(axis='x', rotation=45)\n",
        "    axes[idx].grid(axis='y', alpha=0.3)\n",
        "\n",
        "    # Add value labels on top of bars\n",
        "    for container in axes[idx].containers:\n",
        "        axes[idx].bar_label(container, fmt='%d', padding=3, fontsize=9)\n",
        "\n",
        "plt.suptitle('Distribution of Categorical Features', fontsize=16, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1zgsa5hatmc",
      "metadata": {
        "id": "1zgsa5hatmc"
      },
      "source": [
        "---\n",
        "\n",
        "## 3.5 - EDA Summary & Key Insights\n",
        "\n",
        "Based on the exploratory data analysis, here are the key findings:\n",
        "\n",
        "### Target Variable (country_group)\n",
        " **Class imbalance**: The dataset shows a moderate class imbalance with a 6:1 ratio between largest and smallest country groups.\n",
        "- *Largest class*: Western Europe (most reviews).\n",
        "- *Smallest class*: Eastern Europe (fewest reviews).\n",
        "\n",
        "This imbalance indicates the need for *stratified sampling* during the train/test split to ensure all regions are properly represented in the model.\n",
        "\n",
        "### Data Leakage\n",
        "The features *hotel_city* and *hotel_country* and *all base line scores of hotels* would allow the model to “cheat” by inferring the target from already-known information rather than learning genuine patterns [Since there are 25 unique hotels].\n",
        "\n",
        "###  Numerical Features\n",
        "**Scale**: Most numerical features (review and baseline scores) are on a 0–10 scale.\n",
        "\n",
        "**Distribution**: They are roughly normally distributed with slight right skew (most hotels receive fairly high ratings).\n",
        "\n",
        "**Outliers**: Only a few mild outliers were detected in the box plots.\n",
        "\n",
        "**Variance**: Features show reasonable variance, which is helpful for modeling\n",
        "\n",
        "###  Feature Correlations\n",
        "**Review scores**: Highly correlated with each other (users who rate one aspect highly tend to rate others highly too).\n",
        "\n",
        "**Hotel baseline scores**: Show moderate correlation.\n",
        "\n",
        "###  Categorical Features\n",
        "\n",
        "**User Gender:**\n",
        "- Fairly balanced distribution across Male/Female/Other.\n",
        "- No major gender bias in the dataset.\n",
        "\n",
        "**User Age Group:**\n",
        "- Most users fall in the 25-44 age range\n",
        "- Represents the primary demographic for hotel reviews.\n",
        "\n",
        "**Traveller Type:**\n",
        "- *Couples* and *Families* are the most common (make up about ~60% of reviews).\n",
        "\n",
        "- *Business* and *Solo* travelers are less common but still well-represented.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "hvdfh9tzd6",
      "metadata": {
        "id": "hvdfh9tzd6"
      },
      "source": [
        "---\n",
        "\n",
        "# Section 4: Feature Engineering\n",
        "\n",
        "**Objective:** Create deviation features to capture how individual user experiences differ from hotel baselines.\n",
        "\n",
        "**Approach:** We will use user demographics and deviation features to predict country groups."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "tilofs15xts",
      "metadata": {
        "id": "tilofs15xts"
      },
      "source": [
        "## 4.1 - Deviation Features\n",
        "\n",
        "**Justification:** Deviation features capture how individual user experiences differ from hotel baselines.\n",
        "\n",
        "**Formula:** deviation = individual_review_score - hotel_baseline_score\n",
        "\n",
        "This indicates:\n",
        "- Whether the user's experience was better or worse than the hotel's average\n",
        "- How user satisfaction compares to typical hotel performance\n",
        "- Individual user preferences relative to hotel standards"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "MezM_19Es5h6",
      "metadata": {
        "id": "MezM_19Es5h6"
      },
      "source": [
        "### Computing the Deviation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc7b4b1c2364441d",
      "metadata": {
        "id": "bc7b4b1c2364441d"
      },
      "outputs": [],
      "source": [
        "# Formula: deviation = individual_review_score - hotel_baseline_score\n",
        "\n",
        "df_merged['deviation_cleanliness'] = (\n",
        "    df_merged['review_score_cleanliness'] - df_merged['hotel_cleanliness_base']\n",
        ")\n",
        "\n",
        "df_merged['deviation_comfort'] = (\n",
        "    df_merged['review_score_comfort'] - df_merged['hotel_comfort_base']\n",
        ")\n",
        "\n",
        "df_merged['deviation_facilities'] = (\n",
        "    df_merged['review_score_facilities'] - df_merged['hotel_facilities_base']\n",
        ")\n",
        "\n",
        "df_merged['deviation_location'] = (\n",
        "    df_merged['review_score_location'] - df_merged['hotel_location_base']\n",
        ")\n",
        "\n",
        "df_merged['deviation_staff'] = (\n",
        "    df_merged['review_score_staff'] - df_merged['hotel_staff_base']\n",
        ")\n",
        "\n",
        "df_merged['deviation_value_for_money'] = (\n",
        "    df_merged['review_score_value_for_money'] - df_merged['hotel_value_for_money_base']\n",
        ")\n",
        "\n",
        "deviation_cols = [col for col in df_merged.columns if col.startswith('deviation_')]\n",
        "print(df_merged[deviation_cols].head())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "S7WmWo2itF4F",
      "metadata": {
        "id": "S7WmWo2itF4F"
      },
      "source": [
        "### Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "k6yzzpwei2",
      "metadata": {
        "id": "k6yzzpwei2"
      },
      "outputs": [],
      "source": [
        "df_merged[deviation_cols].describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d39wzfd3pbv",
      "metadata": {
        "id": "d39wzfd3pbv"
      },
      "source": [
        "---\n",
        "\n",
        "# Section 5: Data Preprocessing\n",
        "\n",
        "**Objective:** Prepare data to be ready for our machine learning model through encoding, scaling, and splitting.\n",
        "\n",
        "**Selected Features:**\n",
        "- Categorical: *user_gender*, *user_age_group*, *user_traveller_type* (user demoghraphics)\n",
        "- Numerical: score-based features and the new engineered features *deviation*\n",
        "- Target: *country_group*"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "252b36a9c9252b85",
      "metadata": {
        "id": "252b36a9c9252b85"
      },
      "source": [
        "## 5.0 - Importing libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "694631de3a1ae21c",
      "metadata": {
        "id": "694631de3a1ae21c"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "pyfo1s1s2kl",
      "metadata": {
        "id": "pyfo1s1s2kl"
      },
      "source": [
        "## 5.1 - Feature Selection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "gans7atkjcc",
      "metadata": {
        "id": "gans7atkjcc"
      },
      "outputs": [],
      "source": [
        "df_processed = df_merged.copy()\n",
        "\n",
        "selected_columns = [\n",
        "    # Categorical features\n",
        "    'user_gender', 'user_age_group', 'user_traveller_type',\n",
        "\n",
        "    # Review scores (excluding overall)\n",
        "    'review_score_cleanliness', 'review_score_comfort',\n",
        "    'review_score_facilities', 'review_score_location',\n",
        "    'review_score_staff', 'review_score_value_for_money',\n",
        "\n",
        "    # Deviation features\n",
        "    'deviation_cleanliness', 'deviation_comfort',\n",
        "    'deviation_facilities', 'deviation_location',\n",
        "    'deviation_staff', 'deviation_value_for_money',\n",
        "\n",
        "    # Target\n",
        "    'country_group'\n",
        "]\n",
        "\n",
        "df_processed = df_processed[selected_columns]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2ypp081chvf",
      "metadata": {
        "id": "2ypp081chvf"
      },
      "source": [
        "## 5.2 - Encoding\n",
        "\n",
        "**Objective**: Converting categorical features into numerical form.\n",
        "\n",
        "*One-hot encoding:* used for unordered (nominal) features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eo2sswfdick",
      "metadata": {
        "id": "eo2sswfdick"
      },
      "outputs": [],
      "source": [
        "df_processed = pd.get_dummies(\n",
        "    df_processed,\n",
        "    columns=['user_gender', 'user_age_group', 'user_traveller_type'],\n",
        "    drop_first=True\n",
        ")\n",
        "\n",
        "print(\"After encoding:\")\n",
        "print(f\"\\nColumn names:\")\n",
        "print(df_processed.columns.tolist())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5t98yx44gpj",
      "metadata": {
        "id": "5t98yx44gpj"
      },
      "source": [
        "## 5.3 - Split Features and Target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "jcojkq139x",
      "metadata": {
        "id": "jcojkq139x"
      },
      "outputs": [],
      "source": [
        "X = df_processed.drop('country_group', axis=1)\n",
        "y = df_processed['country_group']"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8pvhb8gj56h",
      "metadata": {
        "id": "8pvhb8gj56h"
      },
      "source": [
        "## 5.4 - Train-Test Split\n",
        "\n",
        "Split the data into 80% training and 20% test sets using stratified sampling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b2ozbomk2y",
      "metadata": {
        "id": "b2ozbomk2y"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, stratify=y, random_state=42\n",
        ")\n",
        "\n",
        "print(\"Dataset Split:\")\n",
        "print(f\"Train: {X_train.shape} ({len(X_train)/len(X)*100:.1f}%)\")\n",
        "print(f\"Test:  {X_test.shape} ({len(X_test)/len(X)*100:.1f}%)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dejsoihj3ep",
      "metadata": {
        "id": "dejsoihj3ep"
      },
      "source": [
        "## 5.5 - Encode Target Variable\n",
        "\n",
        "Convert country_group labels to numerical format using LabelEncoder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "rbs1r4nnjf",
      "metadata": {
        "id": "rbs1r4nnjf"
      },
      "outputs": [],
      "source": [
        "label_encoder = LabelEncoder()\n",
        "y_train_encoded = label_encoder.fit_transform(y_train)\n",
        "y_test_encoded = label_encoder.transform(y_test)\n",
        "\n",
        "print(\"Class Mapping:\")\n",
        "for i, class_name in enumerate(label_encoder.classes_):\n",
        "    print(f\"{i}: {class_name}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5pl38vwj46b",
      "metadata": {
        "id": "5pl38vwj46b"
      },
      "source": [
        "# Section 6: Model Development\n",
        "\n",
        "**Objective:** Training and evaluating two classification models to predict our target variable (*country_groups*).\n",
        "\n",
        "We implemented:\n",
        "1. **Logistic Regression**: A linear baseline model.\n",
        "2. **Random Forest Classifier (RFC)**: A machine learning model that makes predictions by combining the results of many decision trees."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.0 - Importing Libraries"
      ],
      "metadata": {
        "id": "DgScir8hW3Ye"
      },
      "id": "DgScir8hW3Ye"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "npr4yxw8m5j",
      "metadata": {
        "id": "npr4yxw8m5j"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression  # Linear classification model\n",
        "from sklearn.ensemble import RandomForestClassifier  # Ensemble tree-based model\n",
        "from sklearn.model_selection import GridSearchCV  # Exhaustive search over parameter grid\n",
        "from sklearn.metrics import accuracy_score         # Proportion of correct predictions\n",
        "from sklearn.metrics import classification_report  # Detailed per-class metrics\n",
        "from sklearn.metrics import confusion_matrix       # Matrix of actual vs predicted labels\n",
        "from sklearn.metrics import precision_score        # Precision = TP / (TP + FP)\n",
        "from sklearn.metrics import recall_score           # Recall = TP / (TP + FN)\n",
        "from sklearn.metrics import f1_score               # Harmonic mean of precision and recall"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "254or12c7u",
      "metadata": {
        "id": "254or12c7u"
      },
      "source": [
        "## 6.2 - Logistic Regression\n",
        "\n",
        "**What is Logistic Regression?**\n",
        "- A linear model that predicts class probabilities using the logistic (sigmoid) function.\n",
        "\n",
        "- Simple, fast, and interpretable serving as a strong baseline\n",
        "\n",
        "**Key Parameters:**\n",
        "- `max_iter=1000`: Maximum number of iterations for the optimizer to converge.\n",
        "- `class_weight='balanced'`: Automatically adjusts weights to handle any class imbalance.\n",
        "- `random_state=42`: Ensures reproducibility of results.\n",
        "- `n_jobs=-1`: Uses all available CPU cores for parallel processing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "exwt78j6ute",
      "metadata": {
        "id": "exwt78j6ute"
      },
      "outputs": [],
      "source": [
        "# Initialize the model\n",
        "lr_model = LogisticRegression(\n",
        "    max_iter=1000,              # Maximum iterations\n",
        "    class_weight='balanced',    # Handle any class imbalance automatically\n",
        "    random_state=42,            # For reproducibility\n",
        "    n_jobs=-1                   # Use all CPU cores\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Training"
      ],
      "metadata": {
        "id": "vQtwQOROXrxE"
      },
      "id": "vQtwQOROXrxE"
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "lr_model.fit(X_train, y_train_encoded)"
      ],
      "metadata": {
        "id": "W8eHIAFtYtfQ",
        "outputId": "87cf67a2-0f28-4ca9-b41e-11f5fb123ac7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        }
      },
      "id": "W8eHIAFtYtfQ",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'X_train' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3655844394.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mlr_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_encoded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Testing"
      ],
      "metadata": {
        "id": "kKCmmoAEZQVl"
      },
      "id": "kKCmmoAEZQVl"
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the model\n",
        "y_test_pred_lr = lr_model.predict(X_test)\n",
        "\n",
        "# Calculate performance metrics\n",
        "lr_test_acc = accuracy_score(y_test_encoded, y_test_pred_lr)\n",
        "lr_test_prec = precision_score(y_test_encoded, y_test_pred_lr, average='weighted', zero_division=0)\n",
        "lr_test_rec = recall_score(y_test_encoded, y_test_pred_lr, average='weighted', zero_division=0)\n",
        "lr_test_f1 = f1_score(y_test_encoded, y_test_pred_lr, average='weighted', zero_division=0)\n",
        "\n",
        "print(\"Model Performance\")\n",
        "print(\"\\n\")\n",
        "print(f\"{'Accuracy':<12} {'Precision':<12} {'Recall':<12} {'F1-Score':<12}\")\n",
        "print(\"-\"*80)\n",
        "print(f\"{lr_test_acc:<12.4f} {lr_test_prec:<12.4f} {lr_test_rec:<12.4f} {lr_test_f1:<12.4f}\")"
      ],
      "metadata": {
        "id": "Z-b1hEHZYTpx"
      },
      "id": "Z-b1hEHZYTpx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "9ccshtydti",
      "metadata": {
        "id": "9ccshtydti"
      },
      "source": [
        "## 6.3 - Random Forest Classifier\n",
        "\n",
        "**What is Random Forest?**\n",
        "- An ensemble of decision trees that vote on the final prediction.\n",
        "- Each tree is trained on a random subset of the data.\n",
        "\n",
        "Using *GridSearchCV* to determine the best parameters:\n",
        "\n",
        "**What is GridSearchCV?**\n",
        "- Exhaustively searches through a specified parameter grid.\n",
        "- Performs k-fold cross-validation for each parameter combination (preventing overfitting).\n",
        "- Automatically selects the best hyperparameters based on a scoring metric.\n",
        "\n",
        "**Hyperparameter Grid:**\n",
        "- `n_estimators`: Number of trees in the forest (more trees = better but slower).\n",
        "- `max_depth`: Maximum depth of each tree (controls tree complexity).\n",
        "- `min_samples_split`: Minimum samples required to split a node (prevents overfitting).\n",
        "- `min_samples_leaf`: Minimum samples required at each leaf node (prevents overfitting)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dolk596httl",
      "metadata": {
        "id": "dolk596httl"
      },
      "outputs": [],
      "source": [
        "# Initialize the model\n",
        "rf_base = RandomForestClassifier(\n",
        "    class_weight='balanced',  # Handle class imbalance\n",
        "    random_state=42,          # For reproducibility\n",
        "    n_jobs=-1                 # Use all CPU cores\n",
        ")\n",
        "\n",
        "# GridSearchCV will try all combinations of these parameters\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200],           # Number of trees to test\n",
        "    'max_depth': [10, 15, 20, None],          # Maximum tree depth (None = unlimited)\n",
        "    'min_samples_split': [2, 5, 10],          # Minimum samples to split a node\n",
        "    'min_samples_leaf': [1, 2, 4]             # Minimum samples at leaf nodes\n",
        "}\n",
        "\n",
        "# Initialize GridSearchCV\n",
        "# cv=5 means 5-fold cross-validation: splits training data into 5 parts, trains on 4 parts and validates on 1 part, rotating through all combinations\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=rf_base,           # The model to tune\n",
        "    param_grid=param_grid,       # Parameter combinations to try\n",
        "    cv=5,                        # 5-fold cross-validation\n",
        "    scoring='f1_weighted',       # Optimization metric (weighted F1-score)\n",
        "    n_jobs=-1,                   # Parallel processing\n",
        "    verbose=2                    # Print progress updates\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Training"
      ],
      "metadata": {
        "id": "F3iXeJCrc62b"
      },
      "id": "F3iXeJCrc62b"
    },
    {
      "cell_type": "code",
      "source": [
        "#Train all possible models using the parameter grid\n",
        "grid_search.fit(X_train, y_train_encoded)\n",
        "\n",
        "# Extract the best model found by GridSearchCV\n",
        "rf_model = grid_search.best_estimator_\n",
        "\n",
        "# Display the best parameters found\n",
        "print(\"Best Parameters Found:\")\n",
        "for param, value in grid_search.best_params_.items():\n",
        "    print(f\"  {param}: {value}\")\n",
        "print(f\"\\nBest Cross-Validation F1-Score: {grid_search.best_score_:.4f}\")"
      ],
      "metadata": {
        "id": "evmib9iWdAWl"
      },
      "id": "evmib9iWdAWl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Testing"
      ],
      "metadata": {
        "id": "AP2zLJ6kcBNH"
      },
      "id": "AP2zLJ6kcBNH"
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the model\n",
        "y_test_pred_rf = rf_model.predict(X_test)\n",
        "\n",
        "# Calculate performance metricst\n",
        "rf_test_acc = accuracy_score(y_test_encoded, y_test_pred_rf)\n",
        "rf_test_prec = precision_score(y_test_encoded, y_test_pred_rf, average='weighted', zero_division=0)\n",
        "rf_test_rec = recall_score(y_test_encoded, y_test_pred_rf, average='weighted', zero_division=0)\n",
        "rf_test_f1 = f1_score(y_test_encoded, y_test_pred_rf, average='weighted', zero_division=0)\n",
        "\n",
        "print(\"Model Performance\")\n",
        "print(\"\\n\")\n",
        "print(f\"{'Accuracy':<12} {'Precision':<12} {'Recall':<12} {'F1-Score':<12}\")\n",
        "print(\"-\"*80)\n",
        "print(f\"{rf_test_acc:<12.4f} {rf_test_prec:<12.4f} {rf_test_rec:<12.4f} {rf_test_f1:<12.4f}\")\n"
      ],
      "metadata": {
        "id": "ycMFW06KcExl"
      },
      "id": "ycMFW06KcExl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "419m05flur3",
      "metadata": {
        "id": "419m05flur3"
      },
      "source": [
        "## 6.4 - Understanding GridSearchCV Results\n",
        "\n",
        "### **Overview**\n",
        "\n",
        "GridSearchCV tested **108 different parameter combinations** with **5-fold cross-validation**, resulting in **540 total model trainings**. Here's what that means:\n",
        "\n",
        "**The Process:**\n",
        "1. **Parameter Grid**: We defined 4 hyperparameters with multiple values each:\n",
        "   - `n_estimators`: 3 options [50, 100, 200]\n",
        "   - `max_depth`: 4 options [10, 15, 20, None]\n",
        "   - `min_samples_split`: 3 options [2, 5, 10]\n",
        "   - `min_samples_leaf`: 3 options [1, 2, 4]\n",
        "   - Total combinations: 3 × 4 × 3 × 3 = **108 combinations**\n",
        "\n",
        "2. **5-Fold Cross-Validation**: For each combination:\n",
        "   - Split training data into 5 parts\n",
        "   - Train on 4 parts, validate on 1 part\n",
        "   - Rotate 5 times so each part serves as validation once\n",
        "   - Average the 5 results to get reliable performance estimate\n",
        "\n",
        "3. **Total Work**: 108 combinations × 5 folds = **540 model trainings**\n",
        "\n",
        "---\n",
        "\n",
        "### **Best Parameters Found:**\n",
        "\n",
        "GridSearchCV automatically identified the optimal combination:\n",
        "\n",
        "| Parameter | Best Value | What It Means |\n",
        "|-----------|------------|---------------|\n",
        "| **n_estimators** | 200 | Use 200 trees (maximum we tested) - more trees = more diverse predictions |\n",
        "| **max_depth** | None | Allow trees to grow to unlimited depth - captures complex patterns |\n",
        "| **min_samples_split** | 5 | Need at least 5 samples to split a node - prevents splitting on noise |\n",
        "| **min_samples_leaf** | 1 | Allow leaf nodes with 1 sample - aggressive but balanced by ensemble |\n",
        "\n",
        "**Why These Parameters Work:**\n",
        "- **200 trees**: Our 11-class problem benefits from many diverse decision trees voting together\n",
        "- **Unlimited depth**: With 50,000 training samples, we have enough data to support deep trees without severe overfitting\n",
        "- **min_samples_split=5**: Strikes a balance - not too restrictive (like 10) but prevents splitting on tiny groups (like 2)\n",
        "- **min_samples_leaf=1**: Seems aggressive, but the ensemble of 200 trees averages out individual tree overfitting\n",
        "\n",
        "---\n",
        "\n",
        "### **Cross-Validation F1-Score: 0.9120 (91.20%)**\n",
        "\n",
        "This is **excellent** performance! Here's why this metric matters:\n",
        "\n",
        "**What is F1-Score?**\n",
        "- Harmonic mean of Precision and Recall\n",
        "- Ranges from 0 (worst) to 1 (perfect)\n",
        "- Weighted F1 accounts for class imbalance (our 6:1 ratio)\n",
        "\n",
        "**Why Cross-Validation Score Matters:**\n",
        "- **NOT** based on a single test/validation split\n",
        "- **Averaged** across 5 different validation sets\n",
        "- Proves the model **generalizes well** to unseen data\n",
        "- More reliable than a single train/test split\n",
        "\n",
        "**What 91.20% Means:**\n",
        "- Out of 100 predictions, the model correctly classifies ~91 on average\n",
        "- This is the performance **during training** on held-out validation folds\n",
        "- High confidence the model will perform well on new hotel review data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fd8bdfbdc7b212b",
      "metadata": {
        "id": "fd8bdfbdc7b212b"
      },
      "source": [
        "# Section 7: Model Evaluation\n",
        "\n",
        "**Objective:** Evaluate the best performing model (Random Forest with optimized hyperparameters) using the traditional evaluation metrics.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "conf_matrix_section",
      "metadata": {
        "id": "conf_matrix_section"
      },
      "source": [
        "## 7.1 - Confusion Matrix Analysis\n",
        "\n",
        "Visualize the confusion matrix to understand where the Random Forest model makes correct predictions and misclassifications."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "conf_matrix_code",
      "metadata": {
        "id": "conf_matrix_code"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 8))\n",
        "y_test_pred = rf_model.predict(X_test)\n",
        "cm = confusion_matrix(y_test_encoded, y_test_pred)\n",
        "class_names = label_encoder.classes_\n",
        "\n",
        "# Plot\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
        "            xticklabels=class_names, yticklabels=class_names,\n",
        "            cbar_kws={'label': 'Number of Predictions'})\n",
        "plt.title(\"Random Forest - Confusion Matrix\", fontsize=14, fontweight=\"bold\", pad=15)\n",
        "plt.xlabel(\"Predicted Country Group\", fontsize=12)\n",
        "plt.ylabel(\"True Country Group\", fontsize=12)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.yticks(rotation=0)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nClass Labels Mapping:\")\n",
        "for i, class_name in enumerate(label_encoder.classes_):\n",
        "    print(f\"  {i}: {class_name}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "class_report_section",
      "metadata": {
        "id": "class_report_section"
      },
      "source": [
        "## 7.2 - Classification Report\n",
        "\n",
        "Per-class performance metrics showing precision, recall, and F1-score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "class_report_rf",
      "metadata": {
        "id": "class_report_rf"
      },
      "outputs": [],
      "source": [
        "y_test_pred_rf = rf_model.predict(X_test)\n",
        "print(classification_report(\n",
        "    y_test_encoded, y_test_pred_rf,\n",
        "    labels=present,\n",
        "    target_names=present_names\n",
        "))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eval_summary",
      "metadata": {
        "id": "eval_summary"
      },
      "source": [
        "## 7.4 - Evaluation Summary\n",
        "\n",
        "### **Model Performance:**\n",
        "\n",
        "The Random Forest model achieved:\n",
        "- **Test Accuracy: ~90%** - Excellent for an 11-class classification problem."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "lmbkrjast4",
      "metadata": {
        "id": "lmbkrjast4"
      },
      "source": [
        "---\n",
        "\n",
        "# Section 8: Model Explainability (XAI)\n",
        "\n",
        "**Objective:** Use SHAP and LIME to explain model predictions and identify important features.\n",
        "\n",
        "**Why XAI Matters:**\n",
        "- Understand which features drive predictions.\n",
        "- Build trust in the model's decision-making process.\n",
        "- Identify potential biases or unexpected patterns.\n",
        "\n",
        "**Tools:**\n",
        "- **SHAP (SHapley Additive exPlanations)**: *Game theory-based approach* for global and local explanations.\n",
        "- **LIME (Local Interpretable Model-agnostic Explanations)**: Local *surrogate models* for individual predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8.1 SHAP"
      ],
      "metadata": {
        "id": "zufW_k3YhKtm"
      },
      "id": "zufW_k3YhKtm"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "61msjxr4vsy",
      "metadata": {
        "id": "61msjxr4vsy"
      },
      "outputs": [],
      "source": [
        "# - SHAP Analysis -\n",
        "# !pip install shap\n",
        "import shap\n",
        "import numpy as np\n",
        "\n",
        "# Background sample (reference distribution)\n",
        "background = shap.sample(X_train, 100, random_state=42)\n",
        "# Initialize the SHAP explainer\n",
        "explainer = shap.TreeExplainer(rf_model, background)\n",
        "# Sample your dataset\n",
        "sample_size = 500\n",
        "X_test_sample = shap.sample(X_test, sample_size, random_state=42)\n",
        "# Compute SHAP values on the sample only\n",
        "shap_values = explainer.shap_values(X_test_sample)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "h07w1y6oyoo",
      "metadata": {
        "id": "h07w1y6oyoo"
      },
      "source": [
        "### 8.1.1 - Global feature importance with SHAP\n",
        "\n",
        "**What is Global Feature Importance?**\n",
        "- Shows which features are most important across ALL predictions.\n",
        "- Helps identify the most influential features in the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ym3tox2hzhe",
      "metadata": {
        "id": "ym3tox2hzhe"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 8))\n",
        "shap.summary_plot(shap_values, X_test, plot_type=\"bar\", show=False)\n",
        "plt.title(\"SHAP Bar Plot\", fontsize=14, fontweight='bold', pad=20)\n",
        "plt.xlabel(\"Mean |SHAP Value|\", fontsize=11)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "zd55jn7vn1i",
      "metadata": {
        "id": "zd55jn7vn1i"
      },
      "source": [
        "### 8.1.2 - Local Feature Importance with SHAP.\n",
        "\n",
        "**What is Local Feature Importance?**\n",
        "- Explain WHY the model made a specific prediction for a specific sample.\n",
        "- Show how each feature contributed to that individual prediction.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "y5hixpa00h",
      "metadata": {
        "id": "y5hixpa00h"
      },
      "outputs": [],
      "source": [
        "shap.initjs()\n",
        "\n",
        "sample_index = 50\n",
        "\n",
        "# Get the model prediction and true label\n",
        "pred_class = rf_model.predict(X_test.iloc[[sample_index]])[0]\n",
        "true_class = y_test_encoded[sample_index]\n",
        "\n",
        "pred_name = label_encoder.classes_[pred_class]\n",
        "true_name = label_encoder.classes_[true_class]\n",
        "\n",
        "# Get prediction probabilities\n",
        "pred_proba = rf_model.predict_proba(X_test.iloc[[sample_index]])[0]\n",
        "confidence = pred_proba[pred_class] * 100\n",
        "\n",
        "print(f\"\\nSample {sample_index}:\")\n",
        "print(f\"  True Label: {true_name}\")\n",
        "print(f\"  Predicted: {pred_name} (Confidence: {confidence:.1f}%)\")\n",
        "\n",
        "# SHAP values for this predicted class\n",
        "shap_vals_for_sample = shap_values[pred_class][sample_index]\n",
        "\n",
        "# Expected value (base value for class)\n",
        "expected_val = explainer.expected_value[pred_class]\n",
        "\n",
        "# Create the force plot\n",
        "shap.force_plot(\n",
        "    expected_val,\n",
        "    shap_vals_for_sample,\n",
        "    X_test.iloc[sample_index],\n",
        "    feature_names=X_test.columns\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2flxt2lkutt",
      "metadata": {
        "id": "2flxt2lkutt"
      },
      "source": [
        "## 8.2 - LIME Analysis\n",
        "\n",
        "**What is LIME?**\n",
        "- Local Interpretable Model-agnostic Explanations.\n",
        "- Creates simple, interpretable models (linear) to explain individual predictions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e7sybe880r",
      "metadata": {
        "id": "e7sybe880r"
      },
      "outputs": [],
      "source": [
        "# Install LIME\n",
        "# !pip install lime\n",
        "import lime\n",
        "import lime.lime_tabular\n",
        "\n",
        "# Initialize LIME explainer\n",
        "explainer_lime = lime.lime_tabular.LimeTabularExplainer(\n",
        "    training_data=X_train.values,              # Training data to understand feature distributions\n",
        "    feature_names=X_train.columns.tolist(),    # Feature names for readable output\n",
        "    class_names=label_encoder.classes_.tolist(),  # All 11 country group names\n",
        "    mode='classification',                      # Classification task\n",
        "    random_state=42                             # For reproducibility\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "u9xvx1vv1li",
      "metadata": {
        "id": "u9xvx1vv1li"
      },
      "source": [
        "### 8.2.1 - LIME Explanations for Individual Samples\n",
        "\n",
        "Explaining the same samples we used for SHAP to compare both methods."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3wr04vcwqyc",
      "metadata": {
        "id": "3wr04vcwqyc"
      },
      "outputs": [],
      "source": [
        "import lime\n",
        "import lime.lime_tabular\n",
        "\n",
        "# Initialize the LIME explainer\n",
        "explainer_lime = lime.lime_tabular.LimeTabularExplainer(\n",
        "    X_train.values,\n",
        "    feature_names=X_train.columns,\n",
        "    class_names=label_encoder.classes_.tolist(),  # use actual class names\n",
        "    discretize_continuous=True\n",
        ")\n",
        "\n",
        "sample_index = 50\n",
        "\n",
        "# Generate LIME explanation\n",
        "explanation = explainer_lime.explain_instance(\n",
        "    data_row=X_test.values[sample_index],          # same row index as SHAP\n",
        "    predict_fn=rf_model.predict_proba,             # use Random Forest probabilities\n",
        "    num_features=10                                # show top 10 contributing features\n",
        ")\n",
        "\n",
        "explanation.show_in_notebook(show_table=True, show_all=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "n1izoezcmuf",
      "metadata": {
        "id": "n1izoezcmuf"
      },
      "source": [
        "### 8.2.2 - SHAP vs LIME Comparison\n",
        "\n",
        "**Key Differences:**\n",
        "\n",
        "| Aspect | SHAP | LIME |\n",
        "|--------|------|------|\n",
        "| **Method** | Game theory (Shapley values) | Local linear approximation |\n",
        "| **Speed** | Slower (exact calculations) | Faster (sampling-based) |\n",
        "| **Consistency** | Guaranteed consistency | May vary between runs |\n",
        "| **Global view** | Yes | No (only local) |\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "hpf975oovf",
      "metadata": {
        "id": "hpf975oovf"
      },
      "source": [
        "---\n",
        "\n",
        "# Section 9: Inference Function\n",
        "\n",
        "**Objective:** Create a function that accepts raw input and returns the model prediction in natural language."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9.1 - Define Inference Function"
      ],
      "metadata": {
        "id": "yiksXVdlr6xl"
      },
      "id": "yiksXVdlr6xl"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "16mg89r8bqhi",
      "metadata": {
        "id": "16mg89r8bqhi"
      },
      "outputs": [],
      "source": [
        "\n",
        "def predict_country_group(\n",
        "    # User demographics\n",
        "    user_gender,           # 'Male', 'Female', or 'Other'\n",
        "    user_age_group,        # '18-24', '25-34', '35-44', '45-54', '55+'\n",
        "    user_traveller_type,   # 'Business', 'Couple', 'Family', 'Solo'\n",
        "\n",
        "    # Review scores (0-10 scale)\n",
        "    review_score_cleanliness,\n",
        "    review_score_comfort,\n",
        "    review_score_facilities,\n",
        "    review_score_location,\n",
        "    review_score_staff,\n",
        "    review_score_value_for_money,\n",
        "\n",
        "    # Hotel baseline scores (0-10 scale)\n",
        "    hotel_cleanliness_base,\n",
        "    hotel_comfort_base,\n",
        "    hotel_facilities_base,\n",
        "    hotel_location_base,\n",
        "    hotel_staff_base,\n",
        "    hotel_value_for_money_base\n",
        "):\n",
        "\n",
        "    # Step 1: Create input dataframe with raw features\n",
        "    input_data = pd.DataFrame({\n",
        "        'user_gender': [user_gender],\n",
        "        'user_age_group': [user_age_group],\n",
        "        'user_traveller_type': [user_traveller_type],\n",
        "        'review_score_cleanliness': [review_score_cleanliness],\n",
        "        'review_score_comfort': [review_score_comfort],\n",
        "        'review_score_facilities': [review_score_facilities],\n",
        "        'review_score_location': [review_score_location],\n",
        "        'review_score_staff': [review_score_staff],\n",
        "        'review_score_value_for_money': [review_score_value_for_money]\n",
        "    })\n",
        "\n",
        "    # Step 2: Feature Engineering - Calculate deviation features\n",
        "    input_data['deviation_cleanliness'] = review_score_cleanliness - hotel_cleanliness_base\n",
        "    input_data['deviation_comfort'] = review_score_comfort - hotel_comfort_base\n",
        "    input_data['deviation_facilities'] = review_score_facilities - hotel_facilities_base\n",
        "    input_data['deviation_location'] = review_score_location - hotel_location_base\n",
        "    input_data['deviation_staff'] = review_score_staff - hotel_staff_base\n",
        "    input_data['deviation_value_for_money'] = review_score_value_for_money - hotel_value_for_money_base\n",
        "\n",
        "    # Step 3: One-hot encoding for categorical features\n",
        "    input_encoded = pd.get_dummies(\n",
        "        input_data,\n",
        "        columns=['user_gender', 'user_age_group', 'user_traveller_type'],\n",
        "        drop_first=True\n",
        "    )\n",
        "\n",
        "    # Step 4: Ensure all columns match training data\n",
        "    for col in X_train.columns:\n",
        "        if col not in input_encoded.columns:\n",
        "            input_encoded[col] = 0\n",
        "\n",
        "    # Reorder columns to match training data\n",
        "    input_encoded = input_encoded[X_train.columns]\n",
        "\n",
        "    # Step 5: Make prediction\n",
        "    prediction = rf_model.predict(input_encoded)[0]\n",
        "    prediction_proba = rf_model.predict_proba(input_encoded)[0]\n",
        "\n",
        "    # Step 6: Get prediction details\n",
        "    predicted_group = label_encoder.classes_[prediction]\n",
        "    confidence = prediction_proba[prediction] * 100\n",
        "\n",
        "    # Get top 3 predictions\n",
        "    top_3_indices = np.argsort(prediction_proba)[-3:][::-1]\n",
        "    top_3_predictions = [\n",
        "        {\n",
        "            'country_group': label_encoder.classes_[idx],\n",
        "            'probability': prediction_proba[idx] * 100\n",
        "        }\n",
        "        for idx in top_3_indices\n",
        "    ]\n",
        "\n",
        "    # Step 7: Create country group to countries mapping for explanation\n",
        "    country_group_map = {\n",
        "        'North_America': 'North America (United States, Canada)',\n",
        "        'Western_Europe': 'Western Europe (Germany, France, UK, Netherlands, Spain, Italy)',\n",
        "        'Eastern_Europe': 'Eastern Europe (Russia)',\n",
        "        'East_Asia': 'East Asia (China, Japan, South Korea)',\n",
        "        'Southeast_Asia': 'Southeast Asia (Thailand, Singapore)',\n",
        "        'Middle_East': 'Middle East (United Arab Emirates, Turkey)',\n",
        "        'Africa': 'Africa (Egypt, Nigeria, South Africa)',\n",
        "        'Oceania': 'Oceania (Australia, New Zealand)',\n",
        "        'South_America': 'South America (Brazil, Argentina)',\n",
        "        'South_Asia': 'South Asia (India)',\n",
        "        'North_America_Mexico': 'Mexico'\n",
        "    }\n",
        "\n",
        "    # Step 8: Generate explanation\n",
        "    explanation = f\"Based on the user profile ({user_gender}, {user_age_group}, {user_traveller_type}) \"\n",
        "    explanation += f\"and review scores, this hotel is most likely located in \"\n",
        "    explanation += f\"{country_group_map.get(predicted_group, predicted_group)}.\"\n",
        "\n",
        "    # Return comprehensive result\n",
        "    return {\n",
        "        'predicted_group': predicted_group,\n",
        "        'predicted_region': country_group_map.get(predicted_group, predicted_group),\n",
        "        'confidence': round(confidence, 2),\n",
        "        'top_3_predictions': top_3_predictions,\n",
        "        'explanation': explanation\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "zi5w4jyxy7",
      "metadata": {
        "id": "zi5w4jyxy7"
      },
      "source": [
        "## 9.2 - Test Inference Function\n",
        "\n",
        "Testing the inference function with different user profiles and scenarios."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e7em3ajp41o",
      "metadata": {
        "id": "0e7em3ajp41o"
      },
      "outputs": [],
      "source": [
        "# Scenario 1: Young couple on vacation with high ratings\n",
        "print(\"Scenario 1: Young Couple - High Satisfaction\")\n",
        "\n",
        "result1 = predict_country_group(\n",
        "    # User demographics\n",
        "    user_gender='Female',\n",
        "    user_age_group='25-34',\n",
        "    user_traveller_type='Couple',\n",
        "\n",
        "    # Review scores (high ratings)\n",
        "    review_score_cleanliness=9.5,\n",
        "    review_score_comfort=9.0,\n",
        "    review_score_facilities=8.5,\n",
        "    review_score_location=9.5,\n",
        "    review_score_staff=9.0,\n",
        "    review_score_value_for_money=8.0,\n",
        "\n",
        "    # Hotel baseline scores (average)\n",
        "    hotel_cleanliness_base=8.0,\n",
        "    hotel_comfort_base=8.0,\n",
        "    hotel_facilities_base=7.5,\n",
        "    hotel_location_base=8.5,\n",
        "    hotel_staff_base=8.0,\n",
        "    hotel_value_for_money_base=7.5\n",
        ")\n",
        "\n",
        "print(f\"\\nPrediction: {result1['predicted_group']}\")\n",
        "print(f\"Region: {result1['predicted_region']}\")\n",
        "print(f\"Confidence: {result1['confidence']}%\")\n",
        "print(f\"\\n{result1['explanation']}\")\n",
        "print(f\"\\nTop 3 Predictions:\")\n",
        "for i, pred in enumerate(result1['top_3_predictions'], 1):\n",
        "    print(f\"  {i}. {pred['country_group']}: {pred['probability']:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ku717vm5ppd",
      "metadata": {
        "id": "ku717vm5ppd"
      },
      "outputs": [],
      "source": [
        "# Scenario 2: Business traveler with mixed reviews\n",
        "print(\"Scenario 2: Business Traveler - Mixed Experience\")\n",
        "\n",
        "result2 = predict_country_group(\n",
        "    # User demographics\n",
        "    user_gender='Male',\n",
        "    user_age_group='35-44',\n",
        "    user_traveller_type='Business',\n",
        "\n",
        "    # Review scores (mixed - some below baseline)\n",
        "    review_score_cleanliness=7.0,\n",
        "    review_score_comfort=8.0,\n",
        "    review_score_facilities=6.5,\n",
        "    review_score_location=9.0,\n",
        "    review_score_staff=7.5,\n",
        "    review_score_value_for_money=6.0,\n",
        "\n",
        "    # Hotel baseline scores\n",
        "    hotel_cleanliness_base=8.5,\n",
        "    hotel_comfort_base=8.0,\n",
        "    hotel_facilities_base=7.5,\n",
        "    hotel_location_base=8.0,\n",
        "    hotel_staff_base=8.5,\n",
        "    hotel_value_for_money_base=7.0\n",
        ")\n",
        "\n",
        "print(f\"\\nPrediction: {result2['predicted_group']}\")\n",
        "print(f\"Region: {result2['predicted_region']}\")\n",
        "print(f\"Confidence: {result2['confidence']}%\")\n",
        "print(f\"\\n{result2['explanation']}\")\n",
        "print(f\"\\nTop 3 Predictions:\")\n",
        "for i, pred in enumerate(result2['top_3_predictions'], 1):\n",
        "    print(f\"  {i}. {pred['country_group']}: {pred['probability']:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "iuacdm52uml",
      "metadata": {
        "id": "iuacdm52uml"
      },
      "outputs": [],
      "source": [
        "# Scenario 3: Family vacation with budget concerns\n",
        "print(\"Scenario 3: Family Travelers - Budget-Conscious\")\n",
        "\n",
        "result3 = predict_country_group(\n",
        "    # User demographics\n",
        "    user_gender='Female',\n",
        "    user_age_group='45-54',\n",
        "    user_traveller_type='Family',\n",
        "\n",
        "    # Review scores (good value for money emphasis)\n",
        "    review_score_cleanliness=8.0,\n",
        "    review_score_comfort=7.5,\n",
        "    review_score_facilities=8.0,\n",
        "    review_score_location=7.0,\n",
        "    review_score_staff=9.0,\n",
        "    review_score_value_for_money=9.5,\n",
        "\n",
        "    # Hotel baseline scores\n",
        "    hotel_cleanliness_base=7.5,\n",
        "    hotel_comfort_base=7.0,\n",
        "    hotel_facilities_base=7.5,\n",
        "    hotel_location_base=7.5,\n",
        "    hotel_staff_base=8.0,\n",
        "    hotel_value_for_money_base=8.0\n",
        ")\n",
        "\n",
        "print(f\"\\nPrediction: {result3['predicted_group']}\")\n",
        "print(f\"Region: {result3['predicted_region']}\")\n",
        "print(f\"Confidence: {result3['confidence']}%\")\n",
        "print(f\"\\n{result3['explanation']}\")\n",
        "print(f\"\\nTop 3 Predictions:\")\n",
        "for i, pred in enumerate(result3['top_3_predictions'], 1):\n",
        "    print(f\"  {i}. {pred['country_group']}: {pred['probability']:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***See you next milestone!***"
      ],
      "metadata": {
        "id": "ownrwxEZtCLn"
      },
      "id": "ownrwxEZtCLn"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}